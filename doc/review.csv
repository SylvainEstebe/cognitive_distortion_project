Date;Paper;Dataset;Tasks;Method;Benchmarc;Taxonomy source
2017;(Simms et al., 2017);The dataset used in the study is a collection of personal blogs from the Tumblr API, filtered using specific tags to extract text applicable to a counseling setting and provide a balanced mix of distorted and undistorted writing. A total of 459 posts were used for analysis, each hand-labeled as exhibiting distorted or undistorted thought patterns. The data consisted of vectors with 93 input attributes and 1 output attribute for machine learning.;to focus on cognitive distortion, collect and label personal blogs, detect cognitive distortions automatically, improve counseling sessions, and follow a traditional machine learning process;The methodology involves collecting personal blogs, hand-labeling them, extracting features using LIWC, applying machine learning, and evaluating the models using stratified 10-fold cross-validation.;;
2018;(Rojas-Barahona et al., 2018);The dataset used in the study is a corpus of 500K written posts from the Koko platform, where users anonymously post their problems and negative takes on those problems, and peers respond with attempts to offer a more positive angle on the problem. Initially, 1000 posts were analyzed to define the ontology, and then 4035 posts were labeled with thinking errors, emotions, and situations. The inter-annotator agreement was calculated using a contingency table for thinking error, emotion, and situation, showing agreement and disagreement between the two annotators, and Cohen's kappa was calculated to discount the possibility that the agreement may happen by chance.;The study objectives are to introduce a new task: understanding of mental health concepts derived from Cognitive Behavioural Therapy (CBT), define a mental health ontology based on CBT principles, annotate a large corpus where mental health phenomena are exhibited, and perform understanding using deep learning and distributed representations.;The methodology used in the study includes the introduction of a new task related to mental health concepts derived from Cognitive Behavioural Therapy, formulation of an ontology based on CBT principles, labeling a high-quality mental health corpus, training distributed representations of words and sentences, investigation of two classification approaches using GRU and CNN models, training of GloVe word embeddings and skip-thought embeddings, and utilization of various techniques to overcome over-fitting. The study also plans to extend the current ontology and provides access to the code on GitHub.;The benchmarks used to evaluate the model or method in the study include comparing the performance of the models to chance, evaluating the F1-measure of the models detecting thinking errors, emotions, and situations under different oversampling ratios, and comparing the performance of the models under different oversampling ratios.;None
2020;(Shickel et al., 2020);Crowd-Dist and MH;To present a machine learning framework for the automatic detection and classification of 15 common cognitive distortions in mental health text using machine learning techniques, to collect novel datasets of real-world cognitive distortion event recollections from crowdsourcing participants and online mental health therapy logs annotated by experts for the presence of cognitive distortions, and to examine a reduced set of clinically recognized cognitive distortions based on unsupervised machine learning techniques.;The methodology involved machine learning for automatic detection and classification of cognitive distortions, with subsequent analysis of discriminative words and phrases as well as unsupervised content-based clustering and topic modeling.;The benchmarks used to evaluate the model or method in the study are the weighted F1 scores of 0.88 for differentiating between distorted and non-distorted passages, and 0.68 in the larger crowdsourced dataset and 0.45 in the smaller online counseling dataset for classifying distorted passages into one of 15 distortion categories.;[16] J. M. Grohol, “15 Common Cognitive Distor-tions,” 2018. [Online]. Available: https://psychcentral.com/lib/ 15-common-cognitive-distortions/
2021;(Shreevastava & Foltz, 2021);"The dataset used in the study is named ""Therapist Q&A"", obtained from the crowd-sourced data science repository, Kaggle. It follows a Question and Answer format and consists of anonymized patient entries with brief descriptions of their circumstance, symptoms, and thoughts, along with responses from licensed therapists. The annotation task focused on the patient's input and identified specific parts of sentences where negative thinking patterns were most evident.";The study objectives are to detect cognitive distortions from natural language text, analyze the linguistic implications of classification tasks of different types of distortions, and implement feedback to support CBT through the detection of cognitive distortions.;The methodology involves using natural language processing techniques to detect cognitive distortions, comparing linguistic features and classification algorithms, and implementing pretrained Sentence-BERT embeddings to train an SVM classifier. The study has two main goals: detecting cognitive distortions from natural language text and analyzing the linguistic implications of classification tasks for different types of distortions. Feature selection is divided into Semantic and Syntactic features, with two different training approaches implemented for each category.;The benchmarks used to evaluate the model or method in the study include the F1-score of 0.79 obtained using pretrained Sentence-BERT embeddings to train an SVM classifier. The classification algorithms were implemented with default hyper-parameter settings using the scikit-learn package.;Burns and Beck (1999).
2021;(Burger et al., 2021);"The ""Dataset"" used in the study is a collection of 1600 thought records comprising 5747 utterances completed by 320 healthy participants on Amazon Mechanical Turk. It includes both closed and open thought records, as well as the results of three mental health questionnaires. The dataset is publicly available from the 4TU.ResearchData repository with the DOI:10.4121/16685347.";The study objectives are to determine the feasibility of automatically scoring the underlying maladaptive schema of a thought record utterance by a machine and to explore potential improvements in automatic schema identification. The study also aims to investigate hypotheses related to psychological theory.;The methodology involves using natural language processing to automatically extract schemas from thought records, utilizing a dataset of 1600 thought records comprising 5747 utterances from 320 healthy participants. The study achieved substantial agreement between raters on manually scoring the utterances and presented the results as a benchmark solution for further research.;;None
2021;(de Toledo Rodriguez et al., 2021);The dataset used in the study is a small Cognitive Behavioural Therapy (CBT) dataset constructed via crowd-sourcing. It contains key information needed to train automated agents in producing CBT-related content, including multiple key value pairs for a single interaction such as situation, emotions, negative thoughts, and rational responses to those thoughts. The data collection process involved gathering a series of negative thoughts that are objectively distorted and using crowdsourcing resources to obtain realistic counter arguments. The dataset was prepared by collecting situations, feelings, and negative thoughts from various sources such as CBT books, forums, and public content aggregators.;The study objectives are to construct a small CBT dataset via crowd-sourcing and to leverage state-of-the-art pre-trained architectures to transform cognitive distortions.;The methodology involved constructing a small CBT dataset via crowd-sourcing, using state-of-the-art pre-trained architectures to transform cognitive distortions, and evaluating the generated responses through automated sentiment analysis and human evaluation.;;Burns, D. 1981
2022;(Ding et al., 2022);"The dataset used in the study is a real-world dataset of text messages between therapists and clients diagnosed with serious mental illness. This dataset was collected from a randomized controlled trial of a community-based text-message intervention for individuals with serious mental illness. The data were collected from 39 participants enrolled in the active intervention arm of the trial between December 2017 and October 2019. The participants engaged in text-message conversations with trained clinicians up to three times a day for 12 weeks. In total, 14,312 messages were exchanged, with 7,354 coming from clients. The dataset was annotated for distorted thinking, with five common distortions selected: Mental Filter (MF), Jumping to Conclusions (JC), Catastrophizing (C), Should Statements (SM), Overgeneralization (O), and an additional label ""Any Distortion"" (AD) was generated based on the assigned distortions. The annotations were performed by two mental health specialists, and any message could be identified as having multiple distortions or no distortions at all, making it a multi-label multiclass problem.";The study objectives are to address the sparsity roblem in detecting cognitive distortions, compare different data augmentation methods, adapt a domain-specific pretrained language model, explore the impact of the hyperparameter α for mixup, and compare the utility of data augmentation strategies and a domain-specific pretrained language model for improving the identification of infrequently observed cognitive distortions.;The methodology used in the study involved addressing the sparsity problem with two approaches: Data Augmentation and Domain-Specific Model. The researchers utilized a real-world dataset of texts between therapists and clients diagnosed with serious mental illness that was annotated for distorted thinking to examine the viability of different data augmentation methods. They found that mixup was helpful for rare classes and showed the highest performance for majority classes and better results for rare classes with the domain-specific pretrained language model, MentalBERT. The researchers also explored the hyperparameter α for mixup and found that a low α setting is helpful for dominant classes, and a high α for rare classes. The main contributions of the study included comparing different augmentation methods in a low-resource dataset and finding improvements with majority classes and that mixup can improve performance for rare classes.;GLUE benchmark;(Burns, 1999)
2022;(Lybarger et al., 2022);;to automatically identify distorted thinking in text-based patient-therapist exchanges, investigate the role of conversation history in distortion prediction, propose approaches for leveraging dynamic conversation context in model training, demonstrate the improvement in distortion prediction performance by including conversation context, achieve performance comparable to inter-rater agreement, and identify the presence of any distorted thinking with relatively high performance at 0.73 F1;The methodology involves automatic identification of distorted thinking in text-based patient-therapist exchanges, leveraging dynamic conversation context in model training, utilizing a corpus of text message exchanges from a randomized controlled trial, annotation for six cognitive distortion types, interpretation of the task as a multi-label binary text classification task, model performance evaluation using nested cross-validation, calculation of pointwise mutual information and conditional probability, utilization of multiple BERT-based architectures, implementation of best performing models from prior work as baselines, involvement of licensed mental health professionals for annotation, and exploration of the clustering of distortions in time and the role of conversation context.;he benchmarks used to evaluate the model or method in the study include the performance of different model configurations with and without conversation context, as well as the comparison of model performance to inter-rater agreement. The paper also demonstrates that including conversation context improves distortion prediction performance, achieving performance levels comparable to inter-rater agreement. The BERT-only model with random length context achieved the best performance for any distortion and catastrophizing, while the BERT-only model with random mask context achieved the best performance for jumping to conclusions. The BERT+LSTM model with fixed context achieved the best performance for unspecified distortions. The dynamic context approaches yielded a statistically significant improvement over the fixed context for the more frequent and higher performing distortions. Overall, the paper emphasizes the importance of modeling conversational context for identifying cognitive distortions in text-based exchanges between patients and therapists.;(Burns, 1980).
2022;(Alhaj et al., 2022);The dataset used in the study is a collection of Twitter data obtained through crawling Twitter using a translated version of the cognitive distortion scale. The initial responses from volunteers expressing distorted thoughts were used to define keyword lists for each category. The dataset was labeled by two reliable annotators in the psychotherapy domain, and only texts that both annotators labeled with the same label were considered for the study. The dataset underwent preprocessing to remove noise typically found in social media text, including diacritics, punctuations, stop words, emojis, non-Arabic characters, and tweets-related noise like mentions, usernames, hashtags, and Twitter handles. Additionally, stemming was applied to reduce derived or inflected words into their related stems. The dataset was collected by crawling social media to overcome constraints.;"To propose a machine learning-based approach to improve cognitive distortions' classification of Arabic content over Twitter.
To utilize the TM algorithm to overcome the shortness of social media text and improve cognitive distortions' classifications.
To suggest an approach for enriching short textual representations to improve cognitive distortions' classification of the Arabic context over Twitter.";The methodology involves utilizing a transformer-based topic modeling algorithm (BERTopic) to enrich the contextual topic embeddings, acquiring additional information for enrichment, defining keyword lists for each category, labeling tweets by reliable annotators, and conducting experiments with random data splitting and averaging results.;F1-score, Precision, Recall, Accuracy;(Bathina et al., 2021 (Rojas-Barahona et al., 2018)
2023;(Chen et al., 2023);cognitive distortion detection dataset proposed by Shreevastava and Foltz (2021) consists of 2,531 examples of patient speech annotated with ten common types of cognitive distortions. The dataset follows an 80%-20% train-test split and has an average speech length of 167.3 tokens. The ten distortion types studied are common and well-studied, with the expectation of more fine-grained and new distortion types being unveiled as psychotherapy develops.;To develop AI assistance for computational psychotherapy using Large Language Models, focusing on cognitive distortion detection and proposing the Diagnosis of Thought (DoT) prompting, with the ultimate goal of enhancing mental health support systems.;Diagnosis of Thought (DoT) LLMs: ChatGPT (gpt-3.5-turbo)3 , Vicuna4 , and GPT-4;The benchmarks used to evaluate the model or method in the study include F-1 for distortion assessment and weighted F-1 for distortion classification, as well as human evaluations by psychotherapy professionals from UpWork. The dataset used for cognitive distortion detection consists of 2,531 examples of patient speech annotated with ten common types of cognitive distortions. The paper also compares the performance of DoT prompting with directly generating the results and Zero-Shot CoT prompting using three recent representative LLMs: ChatGPT, Vicuna, and GPT-4. (confidence: 90);(Shreevastava \& Foltz, 2021) (Judith S Beck. 2020)
2023;(Maddela et al., 2023);PATTERNREFRAME dataset consists of about 10k examples of thoughts containing unhelpful thought patterns conditioned on personas, accompanied by about 27k positive reframes. The dataset is split into training, validation, and test sets, with respective sizes of 1,920 / 961 / 6,807 for thoughts, and 5,249 / 2,623 / 18,635 for reframed thoughts.;The methodology involves the creation of a novel dataset, PATTERNREFRAME, through a four-step data collection process using crowdworkers. The study also leverages modern language models to guide the generation of language towards desired outcomes. The dataset consists of examples of thoughts containing unhelpful thought patterns conditioned on personas, along with proposals of reframing that do not exhibit the patterns.;BART-large, T5-large, R2C2-3B, GPT3.5, RoBERTa-large, Adam optimizer, linear warmup, early stopping, batch sizes, beam search, perplexity, weighted F1 value, computational infrastructure;;(Burns, 1980)
2023;(Wang et al., 2023);"The ""Dataset"" in Bichen Wang, Pengfei Deng, Yanyan Zhao, Bing Qin is the C2D2 dataset, which is the first expert-supervised Chinese Cognitive Distortion Dataset containing 7,500 cognitive distortion thoughts in everyday life scenes. It is publicly accessible and includes 7,500 instances of cognitive distortion thoughts from 450 different scenes. The dataset is divided into three sets for training, validation, and testing purposes, following an 8:1:1 ratio. (confidence: 95)";The study objectives are to introduce the C2D2 dataset, examine the presence of cognitive distortions in social media texts shared by individuals diagnosed with mental disorders, propose the incorporation of cognitive distortion information to enhance the performance of existing models for mental disorder detection, and contribute to a better understanding of how cognitive distortions appear in individuals' language and their impact on mental health.;he methodology involves introducing the C2D2 dataset, examining cognitive distortions in social media texts, proposing incorporation of cognitive distortion information, conducting experiments, developing a model for detection, utilizing an LSTM, and evaluating the incorporation of cognitive distortion information into mental disorder detection.;The benchmarks used to evaluate the model or method in the study include computational techniques for detecting cognitive distortions from language, the C2D2 dataset, finetuned Chinese versions of pretrained language models, few-shot and zero-shot settings for LLM, and the performance of existing mental disorder detection models augmented by integrating cognitive distortions as an additional feature.;"(Beck, 1970, 1979; Ellis)"
2023;(Sharma et al., 2023);The dataset used in the study is a collection of 600 situations, thoughts, and expert-suggested reframes from mental health experts and MHA visitors, including both hypothetical and real-world scenarios, with annotations of the proposed reframing attributes. It was curated from crowdworker reports on Amazon Mechanical Turk and a survey on the MHA website, ensuring broad coverage of relevant topics based on high diversity and manual filtering. The dataset was annotated by 15 mental health practitioners and clinical psychology graduate students, ensuring high-quality reframes and annotations. It is publicly available on GitHub under an academic, attribution-only license.;"The study objectives are to:
- Define a framework of seven linguistic attributes for reframing thoughts based on psychology literature.
- Develop automated metrics to measure these attributes and validate them with expert judgments from mental health practitioners.
- Investigate what constitutes a ""high-quality"" reframe through a randomized field study on a large mental health website with over 2,000 participants.
- Understand what characterizes successful reframing and how language models can assist people in this process.
- Characterize the linguistic attributes of reframed thoughts, collect a dataset of situations, thoughts, and reframes, develop methods to generate reframes and measure and control their attributes, and investigate which linguistic attributes are related to the reframing outcomes of relatability, helpfulness, and memorability.";The methodology involves a human-centered study, development of automated metrics, collection of a dataset, training of a retrieval-enhanced in-context learning model, and deployment of the model on a mental health website for a randomized field study. (confidence: 100);;"(Beck, 1976; Ding et al., 2022)"
2023;(Chen et al., 2023);cognitive distortion detection dataset proposed by Shreevastava and Foltz (2021) consists of 2,531 examples of patient speech annotated with ten common types of cognitive distortions. The dataset follows an 80%-20% train-test split and has an average speech length of 167.3 tokens. The ten distortion types studied are common and well-studied, with the expectation of more fine-grained and new distortion types being unveiled as psychotherapy develops.;To develop AI assistance for computational psychotherapy using Large Language Models, focusing on cognitive distortion detection and proposing the Diagnosis of Thought (DoT) prompting, with the ultimate goal of enhancing mental health support systems.;The methodology used in the study involves introducing the DoT prompting method for cognitive distortion detection, experimenting with LLMs, utilizing a dataset of annotated patient speech, hiring psychotherapy professionals for evaluation, and obtaining IRB approval.;Cognitive distortion detection dataset, Comparison of DoT prompting with other methods, Evaluation metrics (F-1 for distortion assessment and weighted F-1 for distortion classification), Assessment of generated rationales by psychotherapy professionals from UpWork;"(Beck, 2020; Shreevastava and Foltz, 2021)."
2023;(Wang et al., 2023);small publicly available corpus of cognitive distortions, eRisk-2018 dataset, Clpsych-2015 dataset;To use deep learning techniques to detect depression based on cognitive distortion, To examine the relationship between cognitive distortions and adolescent depression using deep learning techniques, To construct a small publicly available corpus of cognitive distortions and extend it to social media data, To combine the two tasks of detecting depression and cognitive distortions to improve the performance and interpretation of depression detection;The methodology involves the application of statistical learning techniques, deep learning methods, and pre-trained models for automatic depression detection. It also integrates the theory of cognitive distortion about depression using advanced language models like BERT. Multi-task architectures and Long short-term memory (LSTM) with Attention are used for aggregating information.;;None
2023;(Babacan et al., 2023);"The ""Dataset"" used in the study includes two datasets: one consisting of 1000 cognitive distortion sentences generated with GPT-4, and the other involving the addition of synthetic data to the dataset created by Shreevastava and Foltz (2021) to examine its impact on the success status of the publicly available cognitive distortion dataset.";The study objectives are to investigate the effectiveness of GPT-4 in generating synthetic data related to cognitive distortions, compare the performance of synthetic data with existing datasets, examine whether the validity and success rates of existing datasets can be increased with synthetic data, and assess the use and effectiveness of synthetic data in the field of clinical psychology, particularly in the detection of cognitive distortions.;The methodology involved utilizing recent advances in deep learning and natural language processing for automatic detection and classification of cognitive distortions, preparing two datasets, and conducting Experiment-1 to investigate the use and effectiveness of synthetic data in the detection of cognitive distortions.;F1 score of 0.79 and accuracy rate of 1.00 were used as benchmarks to evaluate the model's performance in detecting cognitive distortions.;(Burns, 1980).
